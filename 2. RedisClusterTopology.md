# How Redis manages its storage in a distributed storage concept
## 1. Redis Cluster Topology -

- Minimal cluster that works as expected requires to contain at least 3 master nodes in the cluster and Redis recommendation is to have at least one slave for each master.
- Minimum 3 Redis master nodes on separate 3 machines for each Minimum 3 Redis slaves (One replica for each master node), 1 slave per master (to allow minimal fail-over mechanism)

#### Why Do you need a minimum of 3 masters?
During the failure detection, the majority of the master nodes are required to come to an agreement. If there are only 2 masters, say A and B and B failed, then the A master node cannot reach to a decision according to the protocol. The A node needs another third node, say C, to tell A that it also cannot reach B.

## 2.Redis Cluster TCP ports -

Every Redis Cluster node requires two TCP connections open. 
ðŸ‘‰ The normal Redis TCP port used to **serve clients**, for instance letâ€™s take 7000, plus the port obtained by adding 10000 to the data port, so 17000.
ðŸ‘‰ This second high port (In here, 17000) is used for the **Cluster bus**, that is a node-to-node communication channel using a binary protocol. The Cluster bus is used by nodes for failure detection, configuration update, failover authorization and so forth. 
If you donâ€™t open both TCP ports, your cluster will not work as expected. So make sure that you open both ports in your firewall.

## 3.Redis Cluster data sharding -

>Data sharding is method to break up large database in to smaller parts. Itâ€™s a process of breaking up a database across multiple machines to enhance the manageability, availability, performance and load balancing of the application.

ðŸ‘‰ **A Redis cluster automatically splits the data set among master and slave nodes.**

![image](https://user-images.githubusercontent.com/33947539/149379208-4eb9ce6c-283c-4dcc-8086-ba62955c94fb.png)

ðŸ‘‰**Redis Cluster does not use consistent hashing, but a different form of sharding where every key is conceptually part of what we call an hash slot.**

There are 16384 hash slots in Redis Cluster (Redis clustering follows a distributed slot map approach which distribute 16384 slots among Cluster nodes), and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384.

once you add a key to Redis a â€œhas slotâ€ will be calculated and based on this hash slot the key will land on a particular node.

The distributed algorithm that Redis Cluster uses to map keys to hash slots is,

>HASH_SLOT = CRC16(key) mod HASH_SLOTS_NUMBER

CRC stands for Cyclic Redundancy Check.


Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where:
Node A contains hash slots from 0 to 5500.
Node B contains hash slots from 5501 to 11000.
Node C contains hash slots from 11001 to 16383.
This allows to add and remove nodes (scale) in the cluster easily and does not require any downtime.

![image](https://user-images.githubusercontent.com/33947539/149379835-067c911e-7c22-44a9-941c-5d9edf1f3634.png)

## Redis Fail-over Procedure (Redis Cluster master-slave model)

Master-Slave concept in Redis increases data availability by preventing the single point of failure. Every master node in a Redis cluster has at least one slave node (A replica of the Master node). When the Master node fails to operate or becomes unreachable, the cluster will automatically choose its slave node/one of the slave nodes and make that one as the new Master. Therefore, failure in one node will not stop the entire system from working.

#### How to detect node failures?

ðŸ‘‰ **each node knows information about other nodes and they are constantly gossip with each other using a gossip protocol**

So nodes will know if some node is going fail in the cluster. This helps the cluster to take necessary actions so it could avoid single point failure.

![image](https://user-images.githubusercontent.com/33947539/149380904-fd56a637-c3f0-494d-ac9f-c6d9d1eb3bb2.png)

Every node has a unique ID in the cluster. This ID is used to identify each and every node across the entire cluster using the gossip protocol.
So, a node keeps the following information within itself;

>node ID, IP, and port
>a set of flags
>what is the Master of the node if it is flagged as â€œslaveâ€
>last time a node was pinged
>last time the pong was received

When you ping a Redis node, if it is working properly, it will reply with a pong.

![image](https://user-images.githubusercontent.com/33947539/149381967-4a47e0b4-bf6b-4ea2-ab3c-bb818ef74167.png)

- A guesses B is failing, as the latest PING request timed out. A will not take any action without any other hint.
- C sends a PONG to A, with the gossip section containing information about B: C also thinks B is failing.
- At this point A marks B as failed, and notifies the information to all the other nodes in the cluster, that will mark the node as failing.
- If B will ever return back, the first time heâ€™ll ping any node of the cluster, it will be notified to shut down ASAP, as intermitting clients are not good for the clients.

### Replication Vs. Sharding
- Replication is also known as mirroring. It copies all data in the master node to the slave node/s.
- Sharding is also known as partitioning. It splits the data up by key.

![image](https://user-images.githubusercontent.com/33947539/149458846-858150dc-8dd7-4cc9-9b17-15f2e10cfbd1.png)

e.g.
With sharding, keys 1 and 3 are stored on machine A, and keys 2 and 4 on machine B.
With replication, all the keys 1, 2, 3, and 4 are stored on both machine A and B

### Replication Or Clustering?
ðŸ‘‰ If you have more data than RAM in a single machine, use a Redis cluster to shard the data across multiple databases.

ðŸ‘‰ If you have less data than RAM in a machine, set up a master/slave replication with a sentinel in front to handle the failover.

>A sentinel handles health checks of the masters/slaves, and will automatically promote a slave if a master is unreachable. You need to have at least 3 sentinels running so that they can agree on reachability of nodes, and to ensure the sentinels do not have a single point of failure.

We will discuss more on **Sentnels** in the next page. 








